$\tt 2023.4.25~UPD:$ 更新了整数规划的割平面法和分支定界法, 日后会抽时间做一些修补工作, 包括但不限于用更好的方式封装, 精简代码, 以及修改代码的小BUG等.

一些注意事项：

- 有关精度. 涉及到浮点数的运算时, 误差是不可避免的, 而割平面法与分支定界法循环的结束条件涉及到解是否为整数的判断, 可以考虑用经典的浮点数相等的比较方式, 也可以在运算过程中选择合适的位数进行舍入, 不过不管如何设计, 总会存在一些比较病态的问题导致设置的精度对该问题不适用而陷入死循环. 如果不考虑计算开销的话, 可以定义一个有理数类, 或者是直接使用Python封装好的 `Fraction` 模块, 得到更为精确的解.
- 有关分支定界对最优值的维护. Python的 `heapq` 模块与 `numpy` 兼容得不够好, 导致元素为 `numpy` 矩阵的元组在入队时会因元素矩阵维数不一致导致报错, 这一点需要在入队前做一次类型转换(`numpy` 矩阵到 `list` 的转换)来避免, 较为麻烦. 事实上, 由于线性规划部分并没有涉及到微积分的相关运算, 同时预先分配好空间可以避免矩阵的拼接操作, 且若对 STL 的优先队列更为熟悉的话, 用 C/C++ 加上相应的线性规划求解器来实现整数规划的两个算法是更好的选择, 特别是分支定界.

如要测试, 直接运行 `linear_programming/ILP/Gomory.py` 或者是 `linear_programming/ILP/branch_and_bound.py` 即可, 可以采用相应目录下的 `data.txt` 文件进行测试.

------

$\tt 2023.4.18~UPD:$ 已更新线性规划, 同时修正了拟牛顿法不收敛的问题.

虽然黄金分割法不适用于多峰函数, 但是对于测试函数而言, 将步长调整小一些仍然能取得不错的效果, 而使用 `scipy.optimize` 库中的非精确线搜 `line_search()` 效果却不如黄金分割法, 所以目前无约束优化中的一维搜索仍然采用的是黄金分割法. 

初步实现了两阶段单纯形法, 代码会分别输出两个阶段每次迭代的的单纯形表, 主元坐标, 出基列和进基列, 如要测试, 请运行 `linear_programming/test_data.py` 文件, 按照提示的格式(需要注意是**标准形式的线性规划**)输入数据, 比如对于下面的数据:

```
3 5
0 5 1 0 0
6 2 0 1 0
1 1 0 0 1
15 24 5
-2 -1 0 0 0
```

有如下的输出结果:

```
下面是对人工问题的求解:
初始单纯形表为:
[[  0.   5.   1.   0.   0.   1.   0.   0.  15.]
 [  6.   2.   0.   1.   0.   0.   1.   0.  24.]
 [  1.   1.   0.   0.   1.   0.   0.   1.   5.]
 [ -7.  -8.  -1.  -1.  -1.   0.   0.   0. -44.]]
第1轮迭代, 主元的坐标为(1,0),出基列为6,进基列为0,换基操作后的单纯形表为:
[[  0.      5.      1.      0.      0.      1.      0.      0.     15.   ]
 [  1.      0.333   0.      0.167   0.      0.      0.167   0.      4.   ]
 [  0.      0.667   0.     -0.167   1.      0.     -0.167   1.      1.   ]
 [  0.     -5.667  -1.      0.167  -1.      0.      1.167   0.    -16.   ]]
第2轮迭代, 主元的坐标为(2,1),出基列为7,进基列为1,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   0.   -0.25  1.5   1.5 ]
 [ 0.    0.   -1.   -1.25  7.5   0.   -0.25  8.5  -7.5 ]]
第3轮迭代, 主元的坐标为(0,2),出基列为5,进基列为2,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.   -0.    1.    1.    1.   -0.  ]]
第4轮迭代, 主元的坐标为(2,4),出基列为1,进基列为4,换基操作后的单纯形表为:
[[ 0.     5.     1.     0.     0.     1.     0.     0.    15.   ]
 [ 1.     0.333  0.     0.167  0.     0.     0.167  0.     4.   ]
 [ 0.     0.667  0.    -0.167  1.     0.    -0.167  1.     1.   ]
 [ 0.     0.     0.     0.     0.     1.     1.     1.     0.   ]]
此时检验数均非负,找到最优解.
下面是通过人工问题找到基本解后对原问题的解,此时的单纯形表为:
[[ 0.     5.     1.     0.     0.    15.   ]
 [ 1.     0.333  0.     0.167  0.     4.   ]
 [ 0.     0.667  0.    -0.167  1.     1.   ]
 [ 0.    -0.333  0.     0.333  0.     8.   ]]
第1轮迭代, 主元的坐标为(2,1),出基列为4,进基列为1,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.25  0.5   8.5 ]]
此时检验数均非负,找到最优解.
最优解为[3.5 1.5 7.5 0.  0. ],最优值为-8.5
最终的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.25  0.5   8.5 ]]
```

------

本仓库对常见的最优化算法用 Python 进行了实现, 同时会对机器学习的一些算法转化成的优化模型采用一些实际问题的数据进行试验.

如要测试, 请在 `unconstrained_optimization/encapsulation/test_function.py` 文件中定义函数以及初值点. 推荐采用典型的二次型函数, Rosenbrock 函数 $f(x,y)=100(x-y^2)^2+(1-y)^2 ,$ 以及函数 $f(x,y)=\frac{x^2}{5}+\frac{y^2}{10}+\sin(x+y)$ 进行测试. 可视化的例子在 `unconstrained_optimization/first_order_method/example` 中.

目前已经实现的算法已在下文加粗.

> 目前存在的问题:
>
> 在编写时求解最优步长  $\alpha_k=\arg\min_{\alpha>0}f(x_k+\alpha d_k)$ 的算法时采用了黄金分割法, 这是不对的, 黄金分割法只适用于单峰函数, 若使用黄金分割法且初值距极小点较远时, 对于一些病态的函数需要将精度调到 $10^{-10}$ 以下且迭代几百次甚至上千次才会得到比较好的值(迭代过程中步长的变化不是单调的), 后续将采用非精确线搜进行更正.

