$\tt 2023.4.18~UPD:$ 已更新线性规划, 同时修正了拟牛顿法不收敛的问题.

虽然黄金分割法不适用于多峰函数, 但是对于测试函数而言, 将步长调整小一些仍然能取得不错的效果, 而使用 `scipy.optimize` 库中的非精确线搜 `line_search()` 效果却不如黄金分割法, 所以目前无约束优化中的一维搜索仍然采用的是黄金分割法. 

初步实现了两阶段单纯形法, 代码会分别输出两个阶段每次迭代的的单纯形表, 主元坐标, 出基列和进基列, 如要测试, 请运行 `linear_programming/test_data.py` 文件, 按照提示的格式(需要注意是**标准形式的线性规划**)输入数据, 比如对于下面的数据:

```
3 5
0 5 1 0 0
6 2 0 1 0
1 1 0 0 1
15 24 5
-2 -1 0 0 0
```

有如下的输出结果:

```
下面是对人工问题的求解:
初始单纯形表为:
[[  0.   5.   1.   0.   0.   1.   0.   0.  15.]
 [  6.   2.   0.   1.   0.   0.   1.   0.  24.]
 [  1.   1.   0.   0.   1.   0.   0.   1.   5.]
 [ -7.  -8.  -1.  -1.  -1.   0.   0.   0. -44.]]
第1轮迭代, 主元的坐标为(1,0),出基列为6,进基列为0,换基操作后的单纯形表为:
[[  0.      5.      1.      0.      0.      1.      0.      0.     15.   ]
 [  1.      0.333   0.      0.167   0.      0.      0.167   0.      4.   ]
 [  0.      0.667   0.     -0.167   1.      0.     -0.167   1.      1.   ]
 [  0.     -5.667  -1.      0.167  -1.      0.      1.167   0.    -16.   ]]
第2轮迭代, 主元的坐标为(2,1),出基列为7,进基列为1,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   0.   -0.25  1.5   1.5 ]
 [ 0.    0.   -1.   -1.25  7.5   0.   -0.25  8.5  -7.5 ]]
第3轮迭代, 主元的坐标为(0,2),出基列为5,进基列为2,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.   -0.    1.    1.    1.   -0.  ]]
第4轮迭代, 主元的坐标为(2,4),出基列为1,进基列为4,换基操作后的单纯形表为:
[[ 0.     5.     1.     0.     0.     1.     0.     0.    15.   ]
 [ 1.     0.333  0.     0.167  0.     0.     0.167  0.     4.   ]
 [ 0.     0.667  0.    -0.167  1.     0.    -0.167  1.     1.   ]
 [ 0.     0.     0.     0.     0.     1.     1.     1.     0.   ]]
此时检验数均非负,找到最优解.
下面是通过人工问题找到基本解后对原问题的解,此时的单纯形表为:
[[ 0.     5.     1.     0.     0.    15.   ]
 [ 1.     0.333  0.     0.167  0.     4.   ]
 [ 0.     0.667  0.    -0.167  1.     1.   ]
 [ 0.    -0.333  0.     0.333  0.     8.   ]]
第1轮迭代, 主元的坐标为(2,1),出基列为4,进基列为1,换基操作后的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.25  0.5   8.5 ]]
此时检验数均非负,找到最优解.
最优解为[3.5 1.5 7.5 0.  0. ],最优值为-8.5
最终的单纯形表为:
[[ 0.    0.    1.    1.25 -7.5   7.5 ]
 [ 1.    0.    0.    0.25 -0.5   3.5 ]
 [ 0.    1.    0.   -0.25  1.5   1.5 ]
 [ 0.    0.    0.    0.25  0.5   8.5 ]]
```

------

本仓库对常见的最优化算法用 Python 进行了实现, 同时会对机器学习的一些算法转化成的优化模型采用一些实际问题的数据进行试验.

如要测试, 请在 `unconstrained_optimization/encapsulation/test_function.py` 文件中定义函数以及初值点. 推荐采用典型的二次型函数, Rosenbrock 函数 $f(x,y)=100(x-y^2)^2+(1-y)^2 ,$ 以及函数 $f(x,y)=\frac{x^2}{5}+\frac{y^2}{10}+\sin(x+y)$ 进行测试. 可视化的例子在 `unconstrained_optimization/first_order_method/example` 中.

目前已经实现的算法已在下文加粗.

> 目前存在的问题:
>
> 在编写时求解最优步长  $\alpha_k=\arg\min_{\alpha>0}f(x_k+\alpha d_k)$ 的算法时采用了黄金分割法, 这是不对的, 黄金分割法只适用于单峰函数, 若使用黄金分割法且初值距极小点较远时, 对于一些病态的函数需要将精度调到 $10^{-10}$ 以下且迭代几百次甚至上千次才会得到比较好的值(迭代过程中步长的变化不是单调的), 后续将采用非精确线搜进行更正. 

## 优化算法

1: 一维搜索:

- **黄金分割法**
- **斐波那契法**
- 牛顿法
- 割线法
- 非精确线搜

2: 无约束优化:

- **最速下降法**
- **共轭梯度法**
- **牛顿法及其修正**
- **拟牛顿法**

3: 有约束优化:

- 投影法
- 罚函数法

4: 线性规划:

- **两阶段单纯形法**

5: 整数线性规划:

- Gomory 割平面法
- 分支定界

## 机器学习中的优化算法

给定样本 $\{(x_i,y_i)\}_{i=1}^N,x_i\in\mathbb{R}^n,$ 有下面的约束问题:

1: 多元线性回归

$$
\begin{aligned}
&\min_{w\in\mathbb{R}^n} f(w)=\frac{1}{2}\sum_{i=1}^N(w^Tx_i-y_i)^2\\
\end{aligned}
$$

2: 多元线性回归的正则化:

3: 支持向量机

$$
\begin{aligned}
&\min \|w\|^2\\
\text{s.t.}&y^{(i)}(w^Tx^{(i)}+b)\ge 1,i=1,\cdots,m\\
&w\in\mathbb{R}^n\\
&b\in\mathbb{R}
\end{aligned}
$$

$$
\begin{aligned}
&\max W(\alpha)=\sum_{i}\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}\\
\text{s.t. }&\alpha_i\ge 0,i=1,\cdots,m\\
&\sum_{i=1}^m\alpha_iy^{(i)}=0
\end{aligned}
$$

> 待更新.

